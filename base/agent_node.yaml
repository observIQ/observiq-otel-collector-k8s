apiVersion: opentelemetry.io/v1alpha1
kind: OpenTelemetryCollector
metadata:
  name: observiq-node
  namespace: default
spec:
  mode: daemonset
  hostNetwork: true
  config: |
    receivers:
      filelog:
        include:
          - /var/log/containers/*.log
        start_at: end
        exclude:
          # Avoid parsing collectors log to prevent re-parsing parser errors.
          - /var/log/containers/observiq-otel-collector-log-agent-collector*
          # Avoid parsing logging exporter from gateway.
          - /var/log/containers/observiq-otel-collector-gateway-collector*
        poll_interval: 500ms
        operators:
          # Support docker and containerd runtimes, which have different
          # logging formats.
          - type: router
            routes:
              - expr: 'body matches "^[^\\s]+ \\w+ .*"'
                output: containerd_parser
            default: docker_parser

          # The raw message looks like this:
          # {"log":"I0618 14:30:29.641678       1 logs.go:59] http: TLS handshake error from 192.168.49.2:56222: EOF\n","stream":"stderr","time":"2022-06-18T14:30:29.641732743Z"}
          - type: json_parser
            id: docker_parser
            timestamp:
              parse_from: attributes.time
              layout: '%Y-%m-%dT%H:%M:%S.%sZ'
            output: log-to-body

          # The raw message looks like this:
          # 2022-06-18T16:52:59.639114537Z stdout F {"message":"registered Stackdriver tracing","severity":"info","timestamp":"2022-06-18T16:52:59.639034532Z"}
          - id: containerd_parser
            type: regex_parser
            regex: '^(?P<time>[^\s]+) (?P<stream>\w+) (?P<partial>\w)?(?P<log>.*)'
          - type: recombine
            source_identifier: attributes["log.file.name"]
            combine_field: attributes.log
            is_last_entry: "attributes.partial == 'F'"
          - type: remove
            field: attributes.partial
          - id: time_parser_router
            type: router
            routes:
              # Containerd can have a couple timestamp formats depending if the node has local time set
              - output: local_containerd_timestamp_parser
                expr: 'attributes.time != nil and attributes.time matches "^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d{3,9}[\\+-]\\d{2}:\\d{2}"'
              - output: utc_containerd_timestamp_parser
                expr: 'attributes.time != nil and attributes.time matches "^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d{3,9}Z"'
          - type: time_parser
            id: local_containerd_timestamp_parser
            parse_from: attributes.time
            layout: '%Y-%m-%dT%H:%M:%S.%s%j'
            output: log-to-body
          - type: time_parser
            id: utc_containerd_timestamp_parser
            parse_from: attributes.time
            layout: '%Y-%m-%dT%H:%M:%S.%sZ'
            output: log-to-body

          # The raw body does not contain anything useful considering timestamp has been promotoed to
          # the log entries timestamp, therefore we move attributes.log (the actual container log message)
          # to body.
          - type: move
            id: log-to-body
            from: attributes.log
            to: body

          # Detect pod, namespace, and container names from the file name.
          - type: regex_parser
            regex: '^(?P<pod>[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*)_(?P<namespace>[^_]+)_(?P<container>.+)-'
            parse_from: attributes["log.file.name"]
            cache:
              size: 200

          # Semantic conventions for k8s
          # https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/resource/semantic_conventions/k8s.md#kubernetes
          - type: move
            from: attributes.pod
            to: resource["k8s.pod.name"]
          - type: move
            from: attributes.namespace
            to: resource["k8s.namespace.name"]
          - type: move
            from: attributes.container
            to: resource["k8s.container.name"]

      kubeletstats:
        auth_type: serviceAccount
        collection_interval: 60s
        insecure_skip_verify: true
        k8s_api_config:
          auth_type: serviceAccount
        metric_groups:
        - node
        - pod
        - container

    processors:
      resource:
        attributes:
        - key: k8s.cluster.name
          value: "${K8S_CLUSTER}"
          action: upsert

      k8sattributes:
        pod_association:
        - from: resource_attribute
          name: k8s.pod.uid
        filter:
            node_from_env_var: ${K8S_NODE_NAME}

      batch:
        send_batch_max_size: 1000
        send_batch_size: 1000
        timeout: 2s

    exporters:
      otlp:
        endpoint: observiq-gateway:4317
        tls:
          insecure: true

    extensions:
      file_storage:
        directory: /var/lib/observiq/otelcol/node

    service:
      extensions:
        - file_storage
      pipelines:
        metrics:
          receivers:
            - kubeletstats
          processors:
            - resource
            - k8sattributes
            - batch
          exporters:
            - otlp
        logs:
          receivers:
            - filelog
          processors:
            - resource
            - k8sattributes
            - batch
          exporters:
            - otlp

  image: observiq/observiq-otel-collector:1.2.0
  serviceAccount: observiq-otel-collector
  resources:
    requests:
      memory: 250Mi
      cpu: 250m
  volumes:
    - name: varlog
      hostPath:
        path: /var/log
    - name: dockerlogs
      hostPath:
        path: /var/lib/docker/containers
    - name: storage
      hostPath:
        path: /var/lib/observiq/otelcol/node
  volumeMounts:
    - mountPath: /var/log
      name: varlog
    - mountPath: /var/lib/docker/containers
      name: dockerlogs
    - mountPath: /var/lib/observiq/otelcol/node
      name: storage
      readOnly: false
  env:
    - name: K8S_NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    - name: K8S_CLUSTER
      value: "cluster"
  podSecurityContext:
    runAsUser: 0
